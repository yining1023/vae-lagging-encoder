{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a poetry corpus for training\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "First, install `bpemb`. (I didn't end up using the pretrained embeddings. It's just a convenient way to break a text up with a fixed vocabulary length and avoid ugly out-of-vocab problems.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bpemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `BPEmb` with the desired vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpemb\n",
    "import json, gzip, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = bpemb.BPEmb(lang='en', dim=100, vs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [Project Gutenberg Poetry Corpus](https://github.com/aparrish/gutenberg-poetry-corpus) and change the path below to its location on your drive. Adjust the inequality expression until you have more or less the number of lines you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for line in gzip.open(\"/Users/allison/projects/gutenberg-dammit-archive/gutenberg-poetry-v001.ndjson.gz\", encoding="utf-8"):\n",
    "    data = json.loads(line.strip())\n",
    "    if random.random() < 0.05:\n",
    "        lines.append(data['s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153697"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and grab the first 100k + 10k (validation) + 10k (test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[:120000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then write out the files, using `bpemb` to encode to the fixed vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets/poetry_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_sample_data/poetry_sample.train.txt\", \"w\") as fh:\n",
    "    for line in lines[:100000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_sample_data/poetry_sample.valid.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for line in lines[100000:110000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_sample_data/poetry_sample.test.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for line in lines[110000:120000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_sample_data/vocab.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for item in bp.words:\n",
    "        fh.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train the model as normal with this data. Create a file `config/config_poetry_sample.py` with your desired hyperparameters and then train with the commands discussed in the README. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing with 500k lines instead\n",
    "\n",
    "This is what I did to create the dataset used to train the model included with this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for line in gzip.open(\"/Users/allison/projects/gutenberg-dammit-archive/gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    data = json.loads(line.strip())\n",
    "    if random.random() < 0.2:\n",
    "        lines.append(data['s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[:600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets/poetry_500k_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_500k_sample_data/poetry_500k_sample.train.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for line in lines[:500000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_500k_sample_data/poetry_500k_sample.valid.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for line in lines[500000:550000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_500k_sample_data/poetry_500k_sample.test.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for line in lines[550000:600000]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/poetry_500k_sample_data/vocab.txt\", \"w\", encoding="utf-8") as fh:\n",
    "    for item in bp.words:\n",
    "        fh.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
